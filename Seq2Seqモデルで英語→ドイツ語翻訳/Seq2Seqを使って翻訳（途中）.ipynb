{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import string\n",
    "import requests\n",
    "import io\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "import pickle\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from zipfile import ZipFile\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 今回はmodelsフォルダのうち翻訳トレーニング用のseq2seqモデルを使用する\n",
    "local_repository = 'temp'\n",
    "\n",
    "if tf.__version__[0]<'1':\n",
    "    from tensorflow.models.rnn.translate import seq2seq_model\n",
    "else:\n",
    "    if not os.path.exists(local_repository):\n",
    "        from git import Repo\n",
    "        tf_model_repository = 'https://github.com/tensorflow/models'\n",
    "        Repo.clone_from(tf_model_repository, local_repository)\n",
    "        \n",
    "        # モデルをダウンロードしたあとは、必要な関数をインポート\n",
    "        sys.path.insert(0, 'temp/tutorials/rnn/translate/')\n",
    "        import seq2seq_model as seq2seq_model\n",
    "        import data_utils as data_utils\n",
    "        \n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメーターの設定\n",
    "learning_rate = 0.1#学習率\n",
    "lr_decay_rate = 0.99 # 減衰率\n",
    "lr_decay_every = 100 # 減衰させる頻度\n",
    "max_gradient = 5.0 # 最大勾配\n",
    "batch_size = 50\n",
    "num_layers = 3 # RNNの層の数\n",
    "rnn_size = 500 # RNNモデルのサイズ\n",
    "layer_size = 512\n",
    "generations = 10000\n",
    "vocab_size = 10000\n",
    "save_every = 1000\n",
    "eval_every = 500\n",
    "output_every = 10\n",
    "punct = string.punctuation # 句読点を削除\n",
    "\n",
    "# データのダウンロードと格納\n",
    "data_dir = 'temp'\n",
    "data_file = 'eng_ger.txt'\n",
    "model_path = 'seq2seq_model'\n",
    "full_model_dir = os.path.join(data_dir, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テスト用の英文を３つ設定\n",
    "test_english = ['hello where is my computer',\n",
    "                       'the quick brown fox jumped over the lazy dog',\n",
    "                       'is it going to rain tomorrow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading English-German Data\n",
      "Data not found, downloading Eng-Ger sentences from www.manythings.org\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# データのダウンロード\n",
    "# モデルディレクトリを作成\n",
    "if not os.path.exists(full_model_dir):\n",
    "    os.makedirs(full_model_dir)\n",
    "\n",
    "# データディレクトリを作成\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "    \n",
    "print('Loading English-German Data')\n",
    "# データが存在しない場合、ダウンロードして保存\n",
    "if not os.path.isfile(os.path.join(data_dir, data_file)):\n",
    "    print('Data not found, downloading Eng-Ger sentences from www.manythings.org')\n",
    "    sentence_url = 'http://www.manythings.org/anki/deu-eng.zip'\n",
    "    r = requests.get(sentence_url)\n",
    "    z = ZipFile(io.BytesIO(r.content))\n",
    "    file = z.read('deu.txt')\n",
    "    # データの書式設定\n",
    "    eng_ger_data = file.decode()\n",
    "    eng_ger_data = eng_ger_data.encode('ascii', errors='ignore')\n",
    "    eng_ger_data = eng_ger_data.decode().split('\\n')\n",
    "    # 書き出し\n",
    "    with open(os.path.join(data_dir, data_file), 'w') as out_conn:\n",
    "        for sentence in eng_ger_data:\n",
    "            out_conn.write(sentence + '\\n')\n",
    "else:\n",
    "    eng_ger_data = []\n",
    "    with open(os.path.join(data_dir, data_file), 'r') as in_conn:\n",
    "        for row in in_conn:\n",
    "            eng_ger_data.append(row[:-1])\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの処理\n",
    "# 句読点を削除\n",
    "eng_ger_data = [''.join(char for char in sent if char not in punct) for sent in eng_ger_data]\n",
    "\n",
    "# 各文をタブで分割     \n",
    "eng_ger_data = [x.split('\\t') for x in eng_ger_data if len(x) >= 1]\n",
    "[english_sentence, german_sentence] = [list(x) for x in zip(*eng_ger_data)]\n",
    "english_sentence = [x.lower().split() for x in english_sentence]\n",
    "german_sentence = [x.lower().split() for x in german_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 英語、ドイツ語それぞれ最も出現頻度の高い10000個ずつ辞書作成（残りはラベル０をつける）\n",
    "# 英語の辞書を処理\n",
    "all_english_words = [word for sentence in english_sentence for word in sentence]\n",
    "all_english_counts = Counter(all_english_words)\n",
    "\n",
    "# -1は0(unknown)の分を引くため\n",
    "eng_word_keys = [x[0] for x in all_english_counts.most_common(vocab_size-3)]  # -3 because UNK, S, /S is also in there\n",
    "eng_vocab2ix = dict(zip(eng_word_keys, range(1, vocab_size)))\n",
    "eng_ix2vocab = {val: key for key, val in eng_vocab2ix.items()}\n",
    "english_processed = []\n",
    "for sent in english_sentence:\n",
    "    temp_sentence = []\n",
    "    for word in sent:\n",
    "        try:\n",
    "            temp_sentence.append(eng_vocab2ix[word])\n",
    "        except KeyError:\n",
    "            temp_sentence.append(0)\n",
    "    english_processed.append(temp_sentence)\n",
    "\n",
    "\n",
    "# ドイツ語の辞書を処理\n",
    "all_german_words = [word for sentence in german_sentence for word in sentence]\n",
    "all_german_counts = Counter(all_german_words)\n",
    "ger_word_keys = [x[0] for x in all_german_counts.most_common(vocab_size-3)]  # -3 because UNK, S, /S is also in there\n",
    "ger_vocab2ix = dict(zip(ger_word_keys, range(1, vocab_size)))\n",
    "ger_ix2vocab = {val: key for key, val in ger_vocab2ix.items()}\n",
    "german_processed = []\n",
    "for sent in german_sentence:\n",
    "    temp_sentence = []\n",
    "    for word in sent:\n",
    "        try:\n",
    "            temp_sentence.append(ger_vocab2ix[word])\n",
    "        except KeyError:\n",
    "            temp_sentence.append(0)\n",
    "    german_processed.append(temp_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テスト用の文を、処理\n",
    "test_data = []\n",
    "for sentence in test_english:\n",
    "    temp_sentence = []\n",
    "    for word in sentence.split(\" \"):\n",
    "        try:\n",
    "            temp_sentence.append(eng_vocab2ix[word])\n",
    "        except:\n",
    "            # この単語が辞書に含まれていない場合は0を使用\n",
    "            temp_sentence.append(0)\n",
    "    test_data.append(temp_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 異なる長さの文ごとに別のモデルを作成\n",
    "# シーケンスの長さに基づいてバケットを定義し、データを対応するバケットに分割する\n",
    "x_maxs = [5, 7, 11, 50]\n",
    "y_maxs = [10, 12, 17, 60]\n",
    "buckets = [x for x in zip(x_maxs, y_maxs)]\n",
    "bucketed_data = [[] for _ in range(len(x_maxs))]\n",
    "for eng, ger in zip(english_processed, german_processed):\n",
    "    for ix, (x_max, y_max) in enumerate(zip(x_maxs, y_maxs)):\n",
    "        if (len(eng) <= x_max) and (len(ger) <= y_max):\n",
    "            bucketed_data[ix].append([eng, ger])\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq2Seqとパラメータでモデルを作成するための関数\n",
    "def translation_model(sess, input_vocab_size, output_vocab_size, buckets, rnn_size, num_layers, max_gradient,\n",
    "                     learning_rate, lr_decay_rate, forward_only):\n",
    "    \n",
    "    model = seq2seq_model.Seq2SeqModel(input_vocab_size, output_vocab_size, buckets, rnn_size, num_layers, max_gradient,\n",
    "                                      batch_size, learning_rate, lr_decay_rate, forward_only=forward_only, dtypw=tf.float32)\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'seq2seq_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-05d49e505ef5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                                      \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                      \u001b[0mlr_decay_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr_decay_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                                      forward_only=False)\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# テストモデルで同じ変数を再利用\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-70021eb5b0de>\u001b[0m in \u001b[0;36mtranslation_model\u001b[0;34m(sess, input_vocab_size, output_vocab_size, buckets, rnn_size, num_layers, max_gradient, learning_rate, lr_decay_rate, forward_only)\u001b[0m\n\u001b[1;32m      3\u001b[0m                      learning_rate, lr_decay_rate, forward_only):\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     model = seq2seq_model.Seq2SeqModel(input_vocab_size, output_vocab_size, buckets, rnn_size, num_layers, max_gradient,\n\u001b[0m\u001b[1;32m      6\u001b[0m                                       batch_size, learning_rate, lr_decay_rate, forward_only=forward_only, dtypw=tf.float32)\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'seq2seq_model' is not defined"
     ]
    }
   ],
   "source": [
    "# トレーニング可能なモデルを作成\n",
    "input_vocab_size = vocab_size\n",
    "output_vocab_size = vocab_size\n",
    "\n",
    "translate_model = translation_model(sess=sess, \n",
    "                                     input_vocab_size=vocab_size,\n",
    "                                     output_vocab_size=vocab_size,\n",
    "                                     buckets=buckets,\n",
    "                                     rnn_size=rnn_size,\n",
    "                                     num_layers=num_layers,\n",
    "                                     max_gradient=max_gradient,\n",
    "                                     learning_rate=learning_rate,\n",
    "                                     lr_decay_rate=lr_decay_rate,\n",
    "                                     forward_only=False)\n",
    "\n",
    "# テストモデルで同じ変数を再利用\n",
    "with tf.variable_score(tf.get_variable_scope(), reuse=True):\n",
    "    test_model = translation_model(sess=sess, \n",
    "                                     input_vocab_size=vocab_size,\n",
    "                                     output_vocab_size=vocab_size,\n",
    "                                     buckets=buckets,\n",
    "                                     rnn_size=rnn_size,\n",
    "                                     num_layers=num_layers,\n",
    "                                     max_gradient=max_gradient,\n",
    "                                     learning_rate=learning_rate,\n",
    "                                     lr_decay_rate=lr_decay_rate,\n",
    "                                     forward_only=True)\n",
    "    test_model,batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
